```
Muon(opt = AdamW(eta = 0.0003, beta = (0.9,0.95), lambda = 0.01), η = 0.02, μ = 0.95, λ = 0.01, fallback = Returns(false))
Muon(; [opt, eta, mu, lambda, fallback])
```

Muon - ニュートン・シュルツによって直交化されたモーメンタム (https://github.com/KellerJordan/Muon)

Muonは内部で標準のSGDモーメンタムを実行し、その後、各2Dパラメータの更新をニュートン・シュルツの反復を使用して最も近い直交行列に置き換える直交化後処理ステップを実行します。

# パラメータ

  * フォールバックオプティマイザ (`opt`): 1Dパラメータ用または`fallback`関数がtrueを返す場合に使用するオプティマイザ
  * 学習率 (`η == eta`): 重みを更新する前に勾配が割引かれる量
  * モーメンタム (`μ == mu`): 勾配降下法の顕著な方向への加速を制御します
  * 重み減衰 (`λ == lambda`): $L_2$正則化の強さを制御します。
  * フォールバック関数 (`fallback`): 1D配列に加えてフォールバックオプティマイザを使用すべき時を制御する関数。パラメータ配列が渡され、ブール値を返す必要があります。

注意: 大きなバッチサイズで最も効果的に機能し、ファインチューニングには適さない場合があります。nanoGPTスピードラン実験では、Muonは内部層の>2D重みに使用され、AdamWは1D重み、埋め込み、およびヘッドに使用されます。

`Optimisers.adjust!(optimiser_state, η::Real)`はフォールバックオプティマイザの`eta`を`η * (opt.eta / eta)`に調整し、Muonの`eta`を`η`に調整してその比率を保持しますが、`Optimisers.adjust!(optimiser, eta = η)`はMuonの学習率のみを調整します（フォールバックオプティマイザの学習率を別々に調整できるようにします）。
