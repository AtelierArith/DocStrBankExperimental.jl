```
difference_of_convex_algorithm(M, f, g, ∂h, p=rand(M); kwargs...)
difference_of_convex_algorithm(M, mdco, p; kwargs...)
```

Compute the difference of convex algorithm [BergmannFerreiraSantosSouza:2023](@cite) to minimize

$$
    \operatorname*{arg\,min}_{p∈\mathcal M}\  g(p) - h(p)
$$

where you need to provide $f(p) = g(p) - h(p)$, $g$ and the subdifferential $∂h$ of $h$.

This algorithm performs the following steps given a start point `p`= $p^{(0)}$. Then repeat for $k=0,1,\ldots$

1. Take $X^{(k)}  ∈ ∂h(p^{(k)})$
2. Set the next iterate to the solution of the subproblem

$$
  p^{(k+1)} ∈ \operatorname*{argmin}_{q ∈ \mathcal M} g(q) - ⟨X^{(k)}, \log_{p^{(k)}}q⟩
$$

until the `stopping_criterion` is fulfilled.

# Optional parameters

  * `evaluation`          ([`AllocatingEvaluation`](@ref)) specify whether the gradient works by allocation (default) form `grad_f(M, p)` or [`InplaceEvaluation`](@ref) form `grad_f!(M, X, x)`
  * `gradient`            (`nothing`) specify $\operatorname{grad} f$, for debug / analysis or enhancing `stopping_criterion=`
  * `grad_g`              (`nothing`) specify the gradient of `g`. If specified, a subsolver is automatically set up.
  * `initial_vector`      (`zero_vector(M, p)`) initialise the inner tangent vector to store the subgradient result.
  * `stopping_criterion`  ([`StopAfterIteration`](@ref)`(200) |`[`StopWhenChangeLess`](@ref)`(1e-8)`) a [`StoppingCriterion`](@ref) for the algorithm. This includes a [`StopWhenGradientNormLess`](@ref)`(1e-8)`, when a `gradient` is provided.

if you specify the [`ManifoldDifferenceOfConvexObjective`](@ref) `mdco`, additionally

  * `g`                   - (`nothing`) specify the function `g` If specified, a subsolver is automatically set up.

While there are several parameters for a sub solver, the easiest is to provide the function `grad_g=`, such that together with the mandatory function `g` a default cost and gradient can be generated and passed to a default subsolver. Hence the easiest example call looks like

```
difference_of_convex_algorithm(M, f, g, grad_h, p; grad_g=grad_g)
```

# Optional parameters for the sub problem

  * `sub_cost`              ([`LinearizedDCCost`](@ref)`(g, p, initial_vector)`) a cost to be used within the default `sub_problem` Use this if you have a more efficient version than the default that is built using `g` from before.
  * `sub_grad`              ([`LinearizedDCGrad`](@ref)`(grad_g, p, initial_vector; evaluation=evaluation)` gradient to be used within the default `sub_problem`. This is generated by default when `grad_g` is provided. You can specify your own by overwriting this keyword.
  * `sub_hess`              (a finite difference approximation by default) specify a Hessian  of the subproblem, which the default solver, see `sub_state` needs
  * `sub_kwargs`            (`(;)`) pass keyword arguments to the `sub_state`, in form of a `Dict(:kwname=>value)`, unless you set the `sub_state` directly.
  * `sub_objective`         (a gradient or Hessian objective based on the last 3 keywords) provide the objective used within `sub_problem` (if that is not specified by the user)
  * `sub_problem`           ([`DefaultManoptProblem`](@ref)`(M, sub_objective)` specify a manopt problem for the sub-solver runs. You can also provide a function for a closed form solution. Then `evaluation=` is taken into account for the form of this function.
  * `sub_state`             ([`TrustRegionsState`](@ref) by default, requires `sub_hessian` to be provided; decorated with `sub_kwargs`). Choose the solver by specifying a solver state to solve the `sub_problem` if the `sub_problem` if a function (a closed form solution), this is set to `evaluation` and can be changed to the evaluation type of the closed form solution accordingly.
  * `sub_stopping_criterion` ([`StopAfterIteration`](@ref)`(300) |`[`StopWhenStepsizeLess`](@ref)`(1e-9) |`[`StopWhenGradientNormLess`](@ref)`(1e-9)`) a stopping criterion used withing the default `sub_state=`
  * `sub_stepsize`           ([`ArmijoLinesearch`](@ref)`(M)`) specify a step size used within the `sub_state`,

all others are passed on to decorate the inner [`DifferenceOfConvexState`](@ref).

# Output

the obtained (approximate) minimizer $p^*$, see [`get_solver_return`](@ref) for details
