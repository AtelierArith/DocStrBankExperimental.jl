```
ForwardDiffStepSize1()
```

A `ForwardDiffStepSize` that is derived based on the notes here: https://web.engr.oregonstate.edu/~webbky/MAE4020*5020*files/Section%204%20Roundoff%20and%20Truncation%20Error.pdf. Although it is not often used with Newton-Krylov methods in practice, it can provide some intuition for for how to set the value of `step_adjustment` in a `ForwardDiffJVP`.

The first-order Taylor series expansion of `f(x + ε * Δx)` around `x` is     `f(x + ε * Δx) = f(x) + j(x) * (ε * Δx) + e_trunc(x, ε * Δx)`, where `j(x) = f'(x)` and `e_trunc` is the expansion's truncation error. Due to roundoff error, we are unable to directly compute the value of `f(x)`; instead, we can only determine `f̂(x)`, where     `f(x) = f̂(x) + e_round(x)`. Substituting this into the expansion tells us that     `f̂(x + ε * Δx) + e_round(x + ε * Δx) =         f̂(x) + e_round(x) + j(x) * (ε * Δx) + e_trunc(x, ε * Δx)`. Rearranging this gives us the Jacobian-vector product     `j(x) * Δx = (f̂(x + ε * Δx) - f̂(x)) / ε - e_trunc(x, ε * Δx) / ε +                  (e_round(x + ε * Δx) - e_round(x)) / ε`. So, the normed error of the forward difference approximation of this product is     `‖error‖ = ‖(f̂(x + ε * Δx) - f̂(x)) / ε - j(x) * Δx‖ =              = ‖e_trunc(x, ε * Δx) - e_round(x + ε * Δx) + e_round(x)‖ / ε`. We can use the triangle inequality to get the upper bound     `‖error‖ ≤         (‖e_trunc(x, ε * Δx)‖ + ‖e_round(x + ε * Δx)‖ + ‖e_round(x)‖) / ε`. If `ε` is sufficiently small, we can approximate     `‖e_round(x + ε * Δx)‖ ≈ ‖e_round(x)‖`. This simplifies the upper bound to     `‖error‖ ≤ (‖e_trunc(x, ε * Δx)‖ + 2 * ‖e_round(x)‖) / ε`.

From Taylor's theorem (for multivariate vector-valued functions), the truncation error of the first-order expansion is bounded by     `‖e_trunc(x, ε * Δx)‖ ≤ (sup_{x̂ ∈ X} ‖f''(x̂)‖) / 2 * ‖ε * Δx‖^2`, where `X` is a closed ball around `x` that contains `x + ε * Δx` (see https://math.stackexchange.com/questions/3478229 for a proof of this). Let us define the value     `S = ‖f(x)‖ / sup_{x̂ ∈ X} ‖f''(x̂)‖`. By default, we will assume that `S ≈ 1`, but we will let users pass other values to indicate the "smoothness" of `f(x)` (a large value of `S` should indicate that the Hessian tensor of `f(x)` has a small norm compared to `f(x)` itself). We then have that     `‖e_trunc(x, ε * Δx)‖| ≤ ε^2 / (2 * S) * ‖Δx‖^2 * ‖f(x)‖`.

If only the last bit in each component of `f(x)` can be altered by roundoff error, then the `i`-th component of `e_round(x)` is bounded by     `|e_round(x)[i]| ≤ eps(f(x)[i])`. More generally, we can assume that there is some constant `R` (by default, we will assume that `R ≈ 1`) such that     `|e_round(x)[i]| ≤ R * eps(f(x)[i])`. We can also make the approximation (which is accurate to within `eps(FT)`)     `eps(f(x)[i]) ≈ eps(FT) * |f(x)[i]|`. This implies that     `|e_round(x)[i]| ≤ R * eps(FT) * |f(x)[i]|`. Since this is true for every component of `e_round(x)` and `f(x)`, we find that     `‖e_round(x)‖ ≤ R * eps(FT) * ‖f(x)‖`.

Substituting the bounds on the truncation and roundoff errors into the bound on the overall error gives us     `‖error‖ ≤ ε / (2 * S) * ‖Δx‖^2 * ‖f(x)‖ + 2 / ε * R * eps(FT) * ‖f(x)‖`. Differentiating the right-hand side with respect to `ε` and setting the result equal to 0 (and noting that the second derivative is always positive) tells us that this upper bound is minimized when     `ε = step_adjustment * sqrt(eps(FT)) / ‖Δx‖`, where `step_adjustment = 2 * sqrt(S * R)`. By default, we will assume that `step_adjustment = 1`, but it should be made larger when `f` is very smooth or has a large roundoff error.

Note that, if we were to replace the forward difference approximation in the derivation above with a central difference approximation, the square root would end up being replaced with a cube root (or, more generally, with an `n`-th root for a finite difference approximation of order `n - 1`).
