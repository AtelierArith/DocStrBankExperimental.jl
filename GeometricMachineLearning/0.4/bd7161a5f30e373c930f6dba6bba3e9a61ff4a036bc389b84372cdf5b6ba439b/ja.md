```
AdamOptimizerWithDecay(n_epochs, η₁=1f-2, η₂=1f-6, ρ₁=9f-1, ρ₂=9.9f-1, δ=1f-8)
```

重み減衰を伴うAdamオプティマイザのインスタンスを作成します。

最初の引数（エポック数）を除いて、すべての引数にはデフォルト値があります。

標準の [`AdamOptimizer`](@ref) との違いは、各ステップで学習率 $\eta$ を変更することです。$\eta$ の*時間依存性*を除けば、2つのアルゴリズムは同等です。$\eta(0)$ は高い値 $\eta_1$ から始まり、その後指数関数的に減少して $\eta_2$ に達します。

$$
 \eta(t) = \gamma^t\eta_1,
$$

ここで、$\gamma = \exp(\log(\eta_1 / \eta_2) / \mathtt{n\_epochs})$ です。
