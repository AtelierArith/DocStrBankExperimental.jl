```
run_chat(; model::AbstractString, prompt::AbstractString="", nthreads::Int=Threads.nthreads(), n_gpu_layers::Int=99, ctx_size::Int=2048, args=``)
```

`model`のインタラクティブコンソールを開き、「指示」モードで実行します（特にAlpacaベースのモデルに便利です）。最初のメッセージとしての`prompt`は、今後の対話に関する指示（例：スタイル、トーン、役割）を提供するために使用されることがよくあります。

モデルの応答を待ってから、あなたの返答を入力します。`Enter`を押してメッセージをモデルに送信します。

`Ctrl+C`でチャットを中断します。

詳細については、[完全なドキュメント](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)を参照してください。

# 引数

  * `model`: 使用するモデルへのパス
  * `prompt`: 使用するプロンプト。ほとんどのモデルは、特定の形式での入力を期待します。デフォルトは空の文字列です
  * `nthreads`: 使用するスレッドの数。デフォルトは利用可能なスレッドの数です
  * `n_gpu_layers`: GPUにオフロードするレイヤーの数（llama.cppの`ngl`とも呼ばれます）。GPUのVRAMを多く必要としますが、推論を高速化できます。CPU専用で推論を実行するには0に設定します。デフォルトは99（ほぼすべてのレイヤー）
  * `ctx_size`: コンテキストサイズ、つまりプロンプト/推論がどれだけ大きくできるか。デフォルトは2048（ただし、ほとんどのモデルは4,000以上を許可します）

注意: 奇妙な応答が返ってきた場合、かつ指示調整された（「ファインチューニングされた」）モデルを使用している場合、プロンプトの形式が正しくない可能性があります。正しいプロンプト形式についてはHuggingFaceのモデルドキュメントを参照するか、これを自動で行うライブラリ（例：PromptingTools.jl）を使用してください。

また、`run_llama`、`run_server`も参照してください。
