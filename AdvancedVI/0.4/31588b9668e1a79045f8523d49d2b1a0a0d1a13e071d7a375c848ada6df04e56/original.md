```
optimize(problem, objective, q_init, max_iter, objargs...; kwargs...)
```

Optimize the variational objective `objective` targeting the problem `problem` by estimating (stochastic) gradients.

The trainable parameters in the variational approximation are expected to be extractable through `Optimisers.destructure`. This requires the variational approximation to be marked as a functor through `Functors.@functor`.

# Arguments

  * `objective::AbstractVariationalObjective`: Variational Objective.
  * `q_init`: Initial variational distribution. The variational parameters must be extractable through `Optimisers.destructure`.
  * `max_iter::Int`: Maximum number of iterations.
  * `objargs...`: Arguments to be passed to `objective`.

# Keyword Arguments

  * `adtype::ADtypes.AbstractADType`: Automatic differentiation backend.
  * `optimizer::Optimisers.AbstractRule`: Optimizer used for inference. (Default: `Adam`.)
  * `averager::AbstractAverager` : Parameter averaging strategy. (Default: `NoAveraging()`)
  * `operator::AbstractOperator` : Operator applied to the parameters after each optimization step. (Default: `IdentityOperator()`)
  * `rng::AbstractRNG`: Random number generator. (Default: `Random.default_rng()`.)
  * `show_progress::Bool`: Whether to show the progress bar. (Default: `true`.)
  * `callback`: Callback function called after every iteration. See further information below. (Default: `nothing`.)
  * `prog`: Progress bar configuration. (Default: `ProgressMeter.Progress(n_max_iter; desc="Optimizing", barlen=31, showspeed=true, enabled=prog)`.)
  * `state::NamedTuple`: Initial value for the internal state of optimization. Used to warm-start from the state of a previous run. (See the returned values below.)

# Returns

  * `averaged_params`: Variational parameters generated by the algorithm averaged according to `averager`.
  * `params`: Last variational parameters generated by the algorithm.
  * `stats`: Statistics gathered during optimization.
  * `state`: Collection of the final internal states of optimization. This can used later to warm-start from the last iteration of the corresponding run.

# Callback

The callback function `callback` has a signature of

```
callback(; stat, state, params, averaged_params, restructure, gradient)
```

The arguments are as follows:

  * `stat`: Statistics gathered during the current iteration. The content will vary depending on `objective`.
  * `state`: Collection of the internal states used for optimization.
  * `params`: Variational parameters.
  * `averaged_params`: Variational parameters averaged according to the averaging strategy.
  * `restructure`: Function that restructures the variational approximation from the variational parameters. Calling `restructure(param)` reconstructs the variational approximation.
  * `gradient`: The estimated (possibly stochastic) gradient.

`callback` can return a `NamedTuple` containing some additional information computed within `cb`. This will be appended to the statistic of the current corresponding iteration. Otherwise, just return `nothing`.
