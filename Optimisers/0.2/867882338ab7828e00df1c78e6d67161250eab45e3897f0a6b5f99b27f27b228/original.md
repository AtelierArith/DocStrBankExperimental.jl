```
Descent(η = 1f-1)
```

Classic gradient descent optimiser with learning rate `η`. For each parameter `p` and its gradient `dp`, this runs `p -= η*dp`.

# Parameters

  * Learning rate (`η`): Amount by which gradients are discounted before updating                      the weights.
