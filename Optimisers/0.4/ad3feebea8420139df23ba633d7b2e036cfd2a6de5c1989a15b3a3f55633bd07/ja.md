```
AdamW(η = 0.001, β = (0.9, 0.999), λ = 0, ϵ = 1e-8; couple = true)
AdamW(; [eta, beta, lambda, epsilon, couple])
```

[AdamW](https://arxiv.org/abs/1711.05101) は、重み減衰正則化を修正する（修理する）Adamの変種です。これは、[`OptimiserChain`](@ref) の [`Adam`](@ref) と [`WeightDecay`](@ref) の実装です。

# パラメータ

  * 学習率 (`η == eta`): 勾配が重みを更新する前にどの程度割引かれるかを示します。
  * モーメンタムの減衰 (`β::Tuple == beta`): 最初の（β1）および二番目の（β2）モーメンタム推定のための指数的減衰。
  * 重み減衰 (`λ == lambda`): $L_2$ 正則化の強さを制御します。
  * マシンイプシロン (`ϵ == epsilon`): ゼロ除算を防ぐための定数（デフォルトを変更する必要はありません）
  * キーワード `couple`: `true` の場合、重み減衰は学習率と結合され、pytorchのAdamWのようになります。これは、`x = x - η * (dx + λ * x)` の形の更新に対応し、ここで `dx` は学習率1のAdamからの更新です。`false` の場合、重み減衰は学習率から切り離され、元の論文の精神に従います。これは、`x = x - η * dx - λ * x` の形の更新に対応します。デフォルトは `true` です。

!!! warning "v0.4の破壊的変更"
    バージョン0.4では、AdamWのデフォルトの更新ルールがpytorchの実装に合わせて変更されました。以前のルールは、元の論文に近いもので、`AdamW(..., couple=false)` を設定することで取得できます。詳細については、[この問題](https://github.com/FluxML/Flux.jl/issues/2433)を参照してください。

