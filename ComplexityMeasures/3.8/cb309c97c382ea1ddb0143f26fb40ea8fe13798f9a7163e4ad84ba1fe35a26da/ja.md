```
Renyi <: InformationMeasure
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)
```

レーニー一般化順序-`q` エントロピー [Rényi1961](@cite) は、[`information`](@ref) と共に使用され、`base` で与えられる単位のエントロピーを計算します（通常は `2` または `MathConstants.e`）。

## 説明

$$
p
$$

を確率の配列（合計が 1 になる）とします。すると、レーニー一般化エントロピーは次のようになります。

$$
H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)
$$

これは、情報エントロピー（$q = 1$、[Shannon1948](@citet) を参照）、最大エントロピー（$q=0$、ハートリーエントロピーとも呼ばれる）、または相関エントロピー（$q = 2$、衝突エントロピーとも呼ばれる）など、他の既知のエントロピーを一般化します。

レーニーエントロピーの最大値は $\log_{base}(L)$ であり、これは $L$ が [`total_outcomes`](@ref) である均一分布のエントロピーです。
