```
ADAM(β1 = .99, β2 = .999)
```

A variant of [`SGD`](@ref) with element-wise learning rates generated by exponentially weighted first and second moments of the gradient.

  * Reference: https://ruder.io/optimizing-gradient-descent/
