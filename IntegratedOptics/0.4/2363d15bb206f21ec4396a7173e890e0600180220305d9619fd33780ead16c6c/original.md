```
Descent(η = 0.1)
```

Classic gradient descent optimiser with learning rate `η`. For each parameter `p` and its gradient `δp`, this runs `p -= η*δp`

# Parameters

  * Learning rate (`η`): Amount by which gradients are discounted before updating                      the weights.
