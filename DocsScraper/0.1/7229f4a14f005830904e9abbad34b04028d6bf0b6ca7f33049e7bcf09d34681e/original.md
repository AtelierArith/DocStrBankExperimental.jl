```
make_knowledge_packs(crawlable_urls::Vector{<:AbstractString} = String[];
    single_urls::Vector{<:AbstractString} = String[],
    max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
    model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION, custom_metadata::AbstractString = "",
    embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "",
    target_path::AbstractString = "", save_url_map::Bool = true)
```

Entry point to crawl, parse and generate embeddings. Returns path to tar.gz file of the created index Note: We recommend passing `index_name`. This will be the name of the generated index

# Arguments

  * crawlable_urls: URLs that should be crawled to find more links
  * single_urls: Single page URLs that should just be scraped and parsed. The crawler won't look for more URLs
  * max*chunk*size: Maximum chunk size
  * min*chunk*size: Minimum chunk size
  * model_embedding: Embedding model
  * embedding_dimension: Embedding dimensions
  * custom_metadata: Custom metadata like ecosystem name if required
  * embedding_bool: If true, embeddings generated will be boolean, Float32 otherwise
  * index_name: Name of the index. Default: "index" symbol generated by gensym
  * target_path: Path to the directory where the index folder will be created
  * save*url*map: If true, creates a CSV of crawled URLs with their associated package names
