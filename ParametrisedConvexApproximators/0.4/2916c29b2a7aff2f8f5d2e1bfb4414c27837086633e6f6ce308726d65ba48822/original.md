Max-affine neural network [1].

# Note

If you specify `n` or `m`, it can also be regarded as bivariate function.

# Variables

x ∈ ℝ^n u ∈ ℝ^m z = [xᵀ, uᵀ]ᵀ ∈ ℝ^(n+m) α*is: a vector of subgradients, i.e., α*is[i] ∈ ℝ^(n+m). β*is: a vector of bias terms, i.e., β*is[i] ∈ ℝ.

# References

[1] G. C. Calafiore, S. Gaubert, and C. Possieri, “Log-Sum-Exp Neural Networks and Posynomial Models for Convex and Log-Log-Convex Data,” IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 3, pp. 827–838, Mar. 2020, doi: 10.1109/TNNLS.2019.2910417.
